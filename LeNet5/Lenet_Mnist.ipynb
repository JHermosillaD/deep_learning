{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d98357-f981-43fe-92f2-8105c5ea6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1828950-4d72-4693-ada3-b8436cb69aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\"\n",
    "else: \n",
    "    dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd247b5-6809-4424-a956-4cca0e1586ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        file_output = pickle.load(fo, encoding='bytes')\n",
    "    return file_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beefa23-aeb9-488a-bd5d-a08fb5411b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = load_dataset('/home/jhermosilla/Proyects/Datasets/cifar-100-python/meta')\n",
    "superclasses_set = meta_data[b'coarse_label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3d6ef2-4c66-416e-a0af-b20a4e31bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset('/home/jhermosilla/Proyects/Datasets/cifar-100-python/train')\n",
    "train_images = train_data[b'data']\n",
    "train_images = train_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "train_labels = train_data[b'coarse_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2259baee-aeb2-40b7-922b-374339adb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_dataset('/home/jhermosilla/Proyects/Datasets/cifar-100-python/test')\n",
    "test_images = test_data[b'data']\n",
    "test_images = test_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "test_labels = test_data[b'coarse_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1886e594-d975-46fa-bbcc-1f16126aa9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2Tuple(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.fromarray(image)  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2461bc8-9d91-4e82-8700-4342d4df5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning_class():\n",
    "    def __init__(self, model):\n",
    "        self.epochs = 30\n",
    "        self.device = torch.device(dev)\n",
    "        self.model = model.to(self.device)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=20)\n",
    "        self.accuracy = self.accuracy.to(dev)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        self.model_path = Path(\"models\")\n",
    "        self.model_name = \"lenet5_cifar.pth\"\n",
    "        self.model_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.full_path = self.model_path / self.model_name\n",
    "        self.train_loss_hist = []\n",
    "        self.train_acc_hist = []\n",
    "        self.test_loss_hist = []\n",
    "        self.test_acc_hist = []\n",
    "        self.history = []\n",
    "\n",
    "    def train(self, history = False):\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_acc = 0.0, 0.0\n",
    "            for batch_idx, (images, labels) in enumerate(self.model.train_loader):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                self.model.train()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_func(outputs, labels)\n",
    "                with torch.no_grad():\n",
    "                    train_loss += loss.item()\n",
    "                acc = self.accuracy(outputs, labels)\n",
    "                train_acc += acc\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            train_loss /= len(self.model.train_loader)\n",
    "            train_acc /= len(self.model.train_loader)\n",
    "\n",
    "            test_loss, test_acc = 0.0, 0.0\n",
    "            self.model.eval()\n",
    "            with torch.inference_mode():\n",
    "                for batch_idx, (images, labels) in enumerate(self.model.test_loader):\n",
    "                    images = images.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.loss_func(outputs,labels)\n",
    "                    acc = self.accuracy(outputs,labels)\n",
    "                    with torch.no_grad():\n",
    "                        test_loss += loss.item()\n",
    "                        test_acc += acc\n",
    "                test_loss /= len(self.model.test_loader)\n",
    "                test_acc /= len(self.model.test_loader)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1} Train loss: {train_loss: .5f} Train acc: {train_acc: .5f} Test loss: {test_loss: .5f} Test acc: {test_acc: .5f}\")\n",
    "            if (history):\n",
    "                self.train_loss_hist.append(train_loss)\n",
    "                self.test_loss_hist.append(test_loss)\n",
    "                self.train_acc_hist.append(train_acc.tolist())\n",
    "                self.test_acc_hist.append(test_acc.tolist())\n",
    "\n",
    "        self.history.append(self.train_loss_hist)\n",
    "        self.history.append(self.test_loss_hist)\n",
    "        self.history.append(self.train_acc_hist)\n",
    "        self.history.append(self.test_acc_hist)\n",
    "        return self.model, self.history\n",
    "\n",
    "    def plot_loss(self, history):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot(range(1,self.epochs+1),history[0], label='Train', color='red')\n",
    "        plt.plot(range(1,self.epochs+1),history[1], label='Test', color='green')\n",
    "        plt.title('Loss history')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self, history):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot(range(1,self.epochs+1),history[2], label='Train', color='red')\n",
    "        plt.plot(range(1,self.epochs+1),history[3], label='Test', color='green')\n",
    "        plt.title('Accuracy history')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, trained_model):\n",
    "        print(\"====================================================================================================\")\n",
    "        print(f\"Saving the model: {self.full_path}\")\n",
    "        torch.save(obj=trained_model.state_dict(), f=self.full_path)\n",
    "\n",
    "    def load_model(self, trained_model):\n",
    "        trained_model.load_state_dict(torch.load(self.full_path, weights_only=True))\n",
    "        return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692b34b8-2105-4f53-ace3-c3b563e364e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "learning_model = Learning_class(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bcfa705-ba6d-42ac-842b-672233bb1cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mLearning_class.train\u001b[0;34m(self, history)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     22\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[1;32m     24\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     25\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:192\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    190\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "trained_model, history = learning_model.train(history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf67ab-d127-4b39-a723-1ab69fd203ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
